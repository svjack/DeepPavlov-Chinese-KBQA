{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85390237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModelWithHeads: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from datasets import ClassLabel, load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "import transformers.adapters.composition as ac\n",
    "from transformers import (\n",
    "    AdapterConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    MultiLingAdapterArguments,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "from functools import partial, reduce\n",
    "import json\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from transformers import list_adapters, AutoModelWithHeads\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n",
    "    )\n",
    "    text_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    label_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "            \"efficient on GPU but very bad for TPU.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    label_all_tokens: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n",
    "            \"one (in which case the other tokens will have a padding index).\"\n",
    "        },\n",
    "    )\n",
    "    return_entity_level_metrics: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        self.task_name = self.task_name.lower()\n",
    "\n",
    "with open(\"sel_ner/ner_data_args.pkl\", \"rb\") as f:\n",
    "    t4 = pkl.load(f)\n",
    "\n",
    "model_args, data_args, training_args, adapter_args = map(deepcopy, t4)\n",
    "\n",
    "zh_model = AutoModelWithHeads.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "#config_path = \"/Users/svjack/temp/ner_trans/adapter_ner_data/test-sel-ner/sel_ner/adapter_config.json\"\n",
    "#adapter_path = \"/Users/svjack/temp/ner_trans/adapter_ner_data/test-sel-ner/sel_ner\"\n",
    "config_path = \"sel_ner/adapter_config.json\"\n",
    "adapter_path = \"sel_ner\"\n",
    "\n",
    "config = AdapterConfig.load(config_path)\n",
    "zh_model.load_adapter(adapter_path, config=config)\n",
    "zh_model.set_active_adapters(['sel_ner'])\n",
    "\n",
    "def single_sent_pred(input_text, tokenizer, model):\n",
    "    input_ = tokenizer(input_text)\n",
    "    input_ids = input_[\"input_ids\"]\n",
    "    output = model(torch.Tensor([input_ids]).type(torch.LongTensor))\n",
    "    output_prob = softmax(output.logits.detach().numpy()[0], axis = -1)\n",
    "    token_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    assert len(token_list) == len(output_prob)\n",
    "    return token_list, output_prob\n",
    "\n",
    "def single_pred_to_df(token_list, output_prob, label_list):\n",
    "    assert output_prob.shape[0] == len(token_list) and output_prob.shape[1] == len(label_list)\n",
    "    pred_label_list = pd.Series(output_prob.argmax(axis = -1)).map(\n",
    "        lambda idx: label_list[idx]\n",
    "    ).tolist()\n",
    "    return pd.concat(list(map(pd.Series, [token_list, pred_label_list])), axis = 1)\n",
    "\n",
    "def token_l_to_nest_l(token_l, prefix = \"##\"):\n",
    "    req = []\n",
    "    #req.append([])\n",
    "    #### token_l must startswith [CLS]\n",
    "    assert token_l[0] == \"[CLS]\"\n",
    "    for ele in token_l:\n",
    "        if not ele.startswith(prefix):\n",
    "            req.append([ele])\n",
    "        else:\n",
    "            req[-1].append(ele)\n",
    "    return req\n",
    "\n",
    "def list_window_collect(l, w_size = 1, drop_NONE = False):\n",
    "    assert len(l) >= w_size\n",
    "    req = []\n",
    "    for i in range(len(l)):\n",
    "        l_slice = l[i: i + w_size]\n",
    "        l_slice += [None] * (w_size - len(l_slice))\n",
    "        req.append(l_slice)\n",
    "    if drop_NONE:\n",
    "        return list(filter(lambda x: None not in x, req))\n",
    "    return req\n",
    "\n",
    "def same_pkt_l(l0, l1):\n",
    "    l0_size_l = list(map(len, l0))\n",
    "    assert sum(l0_size_l) == len(l1)\n",
    "    cum_l0_size = np.cumsum(l0_size_l).tolist()\n",
    "    slice_l = list_window_collect(cum_l0_size, 2, drop_NONE=True)\n",
    "    slice_l = [[0 ,slice_l[0][0]]] + slice_l\n",
    "    slice_df = pd.DataFrame(slice_l)\n",
    "    return (l0, slice_df.apply(lambda s: l1[s[0]:s[1]], axis = 1).tolist())\n",
    "\n",
    "\n",
    "def cnt_backtrans_slice(token_list, label_list, prefix = \"##\",\n",
    "    token_agg_func = lambda x: x[0] if len(x) == 1 else \"\".join([x[0]] + list(map(lambda y: y[len(\"##\"):], x[1:]))),\n",
    "    label_agg_func = lambda x: x[0] if len(x) == 1 else pd.Series(x).value_counts().index.tolist()[0]\n",
    "                       ):\n",
    "    token_nest_list = token_l_to_nest_l(token_list, prefix=prefix)\n",
    "    token_nest_list, label_nest_list = same_pkt_l(token_nest_list, label_list)\n",
    "    token_list_req = list(map(token_agg_func, token_nest_list))\n",
    "    label_list_req = list(map(label_agg_func, label_nest_list))\n",
    "    return (token_list_req, label_list_req)\n",
    "\n",
    "def from_text_to_final(input_text, tokenizer, model, label_list):\n",
    "    token_list, output_prob = single_sent_pred(input_text, tokenizer, model)\n",
    "    token_pred_df = single_pred_to_df(token_list, output_prob, label_list)\n",
    "    token_list_, label_list_ = token_pred_df[0].tolist(), token_pred_df[1].tolist()\n",
    "    token_pred_df_reduce = pd.DataFrame(list(zip(*cnt_backtrans_slice(token_list_, label_list_))))\n",
    "    return token_pred_df_reduce\n",
    "\n",
    "\n",
    "tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            tokenizer_name_or_path,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_fast=True,\n",
    "            revision=model_args.model_revision,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "label_list = ['O-TAG', 'E-TAG', 'T-TAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bae710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2348319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### fix eng with \" \"\n",
    "### used when ner_model input with some eng-string fillwith \" \"\n",
    "def fill_str(sent ,str_):\n",
    "    is_en = False\n",
    "    if re.findall(\"[a-zA-Z0-9 ]+\", str_) and re.findall(\"[a-zA-Z0-9 ]+\", str_)[0] == str_:\n",
    "        is_en = True\n",
    "    if not is_en:\n",
    "        return str_\n",
    "    find_part = re.findall(\"([{} ]+)\".format(str_), text)\n",
    "    assert find_part\n",
    "    find_part = sorted(filter(lambda x: x.replace(\" \", \"\") == str_.replace(\" \", \"\") ,find_part), key = len, reverse = True)[0]\n",
    "    assert find_part in sent\n",
    "    return find_part\n",
    "\n",
    "def for_loop_detect(s, invalid_tag = \"O-TAG\", sp_token = \"123454321\"):\n",
    "    assert type(s) == type(pd.Series())\n",
    "    char_list = s.iloc[0]\n",
    "    tag_list = s.iloc[1]\n",
    "    assert len(char_list) == len(tag_list)\n",
    "    req = defaultdict(list)\n",
    "    pre_tag = \"\"\n",
    "    for idx, tag in enumerate(tag_list):\n",
    "        if tag == invalid_tag or tag != pre_tag:\n",
    "            for k in req.keys():\n",
    "                if req[k][-1] != invalid_tag:\n",
    "                    req[k].append(sp_token)\n",
    "            if tag != pre_tag and tag != invalid_tag:\n",
    "                char = char_list[idx]\n",
    "                req[tag].append(char)\n",
    "        elif tag != invalid_tag:\n",
    "            char = char_list[idx]\n",
    "            req[tag].append(char)\n",
    "        pre_tag = tag\n",
    "    req = dict(map(lambda t2: (\n",
    "        t2[0],\n",
    "        list(\n",
    "        filter(lambda x: x.strip() ,\"\".join(t2[1]).split(sp_token))\n",
    "        )\n",
    "                         ), req.items()))\n",
    "    return req\n",
    "\n",
    "def ner_entity_type_predict(question, id_slice_num = 5):\n",
    "    assert type(question) == type(\"\")\n",
    "    question = question.replace(\" \", \"\")\n",
    "    ner_df = from_text_to_final(\n",
    "    \" \".join(list(question)),\n",
    "    tokenizer,\n",
    "    zh_model,\n",
    "    label_list\n",
    ")\n",
    "    assert ner_df.shape[0] == len(question) + 2\n",
    "    ### [UNK] filling\n",
    "    ner_df[0] = [\"[CLS]\"] + list(question) + [\"[SEP]\"]\n",
    "    et_dict = for_loop_detect(ner_df.T.apply(lambda x: x.tolist(), axis = 1))\n",
    "    return et_dict\n",
    "    et_id_dict = dict(\n",
    "map(lambda t2: (\n",
    "    t2[0], list(map(lambda x: np.asarray(x).reshape([-1]).tolist() ,zh_linker_entities(\n",
    "        list(map(lambda x: [x], t2[1]))\n",
    "    )[0]))\n",
    ") ,et_dict.items())\n",
    ")\n",
    "    ori_entity_ids = et_id_dict.get(\"E-TAG\", [])\n",
    "    ori_type_ids = et_id_dict.get(\"T-TAG\", [])\n",
    "\n",
    "    return ori_entity_ids, ori_type_ids, et_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c283f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "def batch_as_list(a, batch_size = int(100000)):\n",
    "    req = []\n",
    "    for ele in a:\n",
    "        if not req:\n",
    "            req.append([])\n",
    "        if len(req[-1]) < batch_size:\n",
    "            req[-1].append(ele)\n",
    "        else:\n",
    "            req.append([])\n",
    "            req[-1].append(ele)\n",
    "    return req\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pkl.dump(obj, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d37e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>O-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>宁</td>\n",
       "      <td>E-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>波</td>\n",
       "      <td>E-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在</td>\n",
       "      <td>O-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>哪</td>\n",
       "      <td>O-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>个</td>\n",
       "      <td>O-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>省</td>\n",
       "      <td>T-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>？</td>\n",
       "      <td>O-TAG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>O-TAG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1\n",
       "0  [CLS]  O-TAG\n",
       "1      宁  E-TAG\n",
       "2      波  E-TAG\n",
       "3      在  O-TAG\n",
       "4      哪  O-TAG\n",
       "5      个  O-TAG\n",
       "6      省  T-TAG\n",
       "7      ？  O-TAG\n",
       "8  [SEP]  O-TAG"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_text_to_final(\"宁波在哪个省？\",\n",
    "    tokenizer,\n",
    "    zh_model,\n",
    "    label_list\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62362426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E-TAG': ['宁波'], 'T-TAG': ['省']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_entity_type_predict(\"宁波在哪个省？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1fb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_cp_with_trans_fullfill = pd.read_csv(\"/Users/svjack/temp/flair_exp/data/kb_re_wiki_qst_gen/df_total_cp_with_trans_fullfill.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "917a1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = df_total_cp_with_trans_fullfill[\"question\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4f3faa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Heart of Stone\"是什么时候发行的?', {'E-TAG': ['HeartofStone']})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = qs.iloc[22].strip()\n",
    "sent, ner_entity_type_predict(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7beabd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"4000年第一女神\"在哪个国家引发了争议?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180399d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c242115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 北京何时获得奥林匹克运动会主办权?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40b4edf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentence</th>\n",
       "      <th>statement</th>\n",
       "      <th>t3_cate</th>\n",
       "      <th>sentence_type</th>\n",
       "      <th>ask_query_spans</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>s_ext</th>\n",
       "      <th>p_ext</th>\n",
       "      <th>o_ext</th>\n",
       "      <th>p_zh_info</th>\n",
       "      <th>ask_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京何时获得奥林匹克运动会主办权?</td>\n",
       "      <td>北京何时获得奥林匹克运动会主办权?</td>\n",
       "      <td>北京2022年获得奥林匹克运动会主办权</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?value</td>\n",
       "      <td>2022年</td>\n",
       "      <td>?s</td>\n",
       "      <td>pq:time_and_date_something_took_place,_existed...</td>\n",
       "      <td>2022年</td>\n",
       "      <td>日期</td>\n",
       "      <td>何时</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>北京何时获得奥林匹克运动会主办权?</td>\n",
       "      <td>北京何时获得奥林匹克运动会主办权?</td>\n",
       "      <td>北京2022年获得奥林匹克运动会主办权</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?value</td>\n",
       "      <td>2022年</td>\n",
       "      <td>?s</td>\n",
       "      <td>ps:sovereign_state_of_this_item_</td>\n",
       "      <td>奥林匹克运动会</td>\n",
       "      <td>国家</td>\n",
       "      <td>何时</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>北京何时获得奥林匹克运动会主办权?</td>\n",
       "      <td>北京何时获得奥林匹克运动会主办权?</td>\n",
       "      <td>北京2022年获得奥林匹克运动会主办权</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?value</td>\n",
       "      <td>2022年</td>\n",
       "      <td>北京</td>\n",
       "      <td>p:sovereign_state_of_this_item_</td>\n",
       "      <td>?s</td>\n",
       "      <td>国家</td>\n",
       "      <td>何时</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>北京何时首次公布其污染日均浓度?</td>\n",
       "      <td>北京何时首次公布其污染日均浓度?</td>\n",
       "      <td>北京2012年2月2日首次公布其污染日均浓度</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?value</td>\n",
       "      <td>2012年2月2日</td>\n",
       "      <td>?s</td>\n",
       "      <td>pq:indicates_the_time_an_item_begins_to_exist_...</td>\n",
       "      <td>2012年2月2日</td>\n",
       "      <td>始於</td>\n",
       "      <td>何时</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北京何时首次公布其污染日均浓度?</td>\n",
       "      <td>北京何时首次公布其污染日均浓度?</td>\n",
       "      <td>北京2012年2月2日首次公布其污染日均浓度</td>\n",
       "      <td>6</td>\n",
       "      <td>special</td>\n",
       "      <td>?value</td>\n",
       "      <td>2012年2月2日</td>\n",
       "      <td>?s</td>\n",
       "      <td>污染日均浓度</td>\n",
       "      <td>?x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>何时</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214985</th>\n",
       "      <td>龙飞船是什么时候提交给NASA的?</td>\n",
       "      <td>龙飞船是什么时候提交给NASA的?</td>\n",
       "      <td>龙飞船是2006年3月3日提交给NASA的</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?sbj</td>\n",
       "      <td>2006年3月3日</td>\n",
       "      <td>2006年3月3日</td>\n",
       "      <td>wdt:to</td>\n",
       "      <td>NASA</td>\n",
       "      <td>終於</td>\n",
       "      <td>什么时候</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214986</th>\n",
       "      <td>龙飞船是哪个国家的?</td>\n",
       "      <td>龙飞船是哪个国家的?</td>\n",
       "      <td>龙飞船是中国的</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?answer</td>\n",
       "      <td>中国</td>\n",
       "      <td>龙飞船</td>\n",
       "      <td>国家的</td>\n",
       "      <td>中国</td>\n",
       "      <td>有效地点</td>\n",
       "      <td>哪个国家</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214987</th>\n",
       "      <td>龙飞船的载人改型是什么?</td>\n",
       "      <td>龙飞船的载人改型是什么?</td>\n",
       "      <td>龙飞船的载人改型是龙飞船2号</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?answer</td>\n",
       "      <td>龙飞船2号</td>\n",
       "      <td>龙飞船</td>\n",
       "      <td>改型</td>\n",
       "      <td>龙飞船2号</td>\n",
       "      <td>NaN</td>\n",
       "      <td>什么</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214988</th>\n",
       "      <td>龙飞船的载人改型是什么时候发射的?</td>\n",
       "      <td>龙飞船的载人改型是什么时候发射的?</td>\n",
       "      <td>龙飞船的载人改型是2020年11月15日发射的</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?sbj</td>\n",
       "      <td>2020年11月15日</td>\n",
       "      <td>2020年11月15日</td>\n",
       "      <td>载人改型</td>\n",
       "      <td>龙飞船</td>\n",
       "      <td>NaN</td>\n",
       "      <td>什么时候</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214989</th>\n",
       "      <td>龙飞船的载人改型是什么时候发射的?</td>\n",
       "      <td>龙飞船的载人改型是什么时候发射的?</td>\n",
       "      <td>龙飞船的载人改型是2020年11月15日发射的</td>\n",
       "      <td>3</td>\n",
       "      <td>special</td>\n",
       "      <td>?sbj</td>\n",
       "      <td>2020年11月15日</td>\n",
       "      <td>2020年11月15日</td>\n",
       "      <td>wdt:not_a_type_but_is_instance_of</td>\n",
       "      <td>载人改型</td>\n",
       "      <td>隶属于</td>\n",
       "      <td>什么时候</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214990 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question           sentence                statement  \\\n",
       "0        北京何时获得奥林匹克运动会主办权?  北京何时获得奥林匹克运动会主办权?      北京2022年获得奥林匹克运动会主办权   \n",
       "1        北京何时获得奥林匹克运动会主办权?  北京何时获得奥林匹克运动会主办权?      北京2022年获得奥林匹克运动会主办权   \n",
       "2        北京何时获得奥林匹克运动会主办权?  北京何时获得奥林匹克运动会主办权?      北京2022年获得奥林匹克运动会主办权   \n",
       "3         北京何时首次公布其污染日均浓度?   北京何时首次公布其污染日均浓度?   北京2012年2月2日首次公布其污染日均浓度   \n",
       "4         北京何时首次公布其污染日均浓度?   北京何时首次公布其污染日均浓度?   北京2012年2月2日首次公布其污染日均浓度   \n",
       "...                    ...                ...                      ...   \n",
       "214985   龙飞船是什么时候提交给NASA的?  龙飞船是什么时候提交给NASA的?    龙飞船是2006年3月3日提交给NASA的   \n",
       "214986          龙飞船是哪个国家的?         龙飞船是哪个国家的?                  龙飞船是中国的   \n",
       "214987        龙飞船的载人改型是什么?       龙飞船的载人改型是什么?           龙飞船的载人改型是龙飞船2号   \n",
       "214988   龙飞船的载人改型是什么时候发射的?  龙飞船的载人改型是什么时候发射的?  龙飞船的载人改型是2020年11月15日发射的   \n",
       "214989   龙飞船的载人改型是什么时候发射的?  龙飞船的载人改型是什么时候发射的?  龙飞船的载人改型是2020年11月15日发射的   \n",
       "\n",
       "        t3_cate sentence_type ask_query_spans  answer_text        s_ext  \\\n",
       "0             3       special          ?value        2022年           ?s   \n",
       "1             3       special          ?value        2022年           ?s   \n",
       "2             3       special          ?value        2022年           北京   \n",
       "3             3       special          ?value    2012年2月2日           ?s   \n",
       "4             6       special          ?value    2012年2月2日           ?s   \n",
       "...         ...           ...             ...          ...          ...   \n",
       "214985        3       special            ?sbj    2006年3月3日    2006年3月3日   \n",
       "214986        3       special         ?answer           中国          龙飞船   \n",
       "214987        3       special         ?answer        龙飞船2号          龙飞船   \n",
       "214988        3       special            ?sbj  2020年11月15日  2020年11月15日   \n",
       "214989        3       special            ?sbj  2020年11月15日  2020年11月15日   \n",
       "\n",
       "                                                    p_ext      o_ext  \\\n",
       "0       pq:time_and_date_something_took_place,_existed...      2022年   \n",
       "1                        ps:sovereign_state_of_this_item_    奥林匹克运动会   \n",
       "2                         p:sovereign_state_of_this_item_         ?s   \n",
       "3       pq:indicates_the_time_an_item_begins_to_exist_...  2012年2月2日   \n",
       "4                                                  污染日均浓度         ?x   \n",
       "...                                                   ...        ...   \n",
       "214985                                             wdt:to       NASA   \n",
       "214986                                                国家的         中国   \n",
       "214987                                                 改型      龙飞船2号   \n",
       "214988                                               载人改型        龙飞船   \n",
       "214989                  wdt:not_a_type_but_is_instance_of       载人改型   \n",
       "\n",
       "       p_zh_info ask_spans  \n",
       "0             日期        何时  \n",
       "1             国家        何时  \n",
       "2             国家        何时  \n",
       "3             始於        何时  \n",
       "4            NaN        何时  \n",
       "...          ...       ...  \n",
       "214985        終於      什么时候  \n",
       "214986      有效地点      哪个国家  \n",
       "214987       NaN        什么  \n",
       "214988       NaN      什么时候  \n",
       "214989       隶属于      什么时候  \n",
       "\n",
       "[214990 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total_cp_with_trans_fullfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b0b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py377",
   "language": "python",
   "name": "py377"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
