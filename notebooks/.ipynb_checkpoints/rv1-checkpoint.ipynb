{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a428975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jieba] default dict file path ../data/vocab.txt\n",
      "[jieba] default dict file path ../data/vocab.txt\n",
      "[jieba] load default dict ../data/vocab.txt ...\n",
      "[jieba] load default dict ../data/vocab.txt ...\n",
      ">> Synonyms load wordseg dict [/opt/conda/envs/kbqa_env/lib/python3.7/site-packages/synonyms/data/vocab.txt] ... \n",
      ">> Synonyms on loading stopwords [/opt/conda/envs/kbqa_env/lib/python3.7/site-packages/synonyms/data/stopwords.txt] ...\n",
      "[Synonyms] on loading vectors [/opt/conda/envs/kbqa_env/lib/python3.7/site-packages/synonyms/data/words.vector.gz] ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kbqa_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from django.shortcuts import render\n",
    "\n",
    "# Create your views here.\n",
    "from django.http import HttpResponse\n",
    "from django.views.decorators.csrf import csrf_exempt\n",
    "#from django.db.transaction import commit_on_success\n",
    "from django.db import transaction\n",
    "\n",
    "'''\n",
    "from .models import Embedding\n",
    "from .models import Translation\n",
    "from .models import Valid\n",
    "'''\n",
    "import json\n",
    "import os\n",
    "\n",
    "#main_path = \"/Volumes/TOSHIBA EXT/temp/kbqa_portable_prj\"\n",
    "#main_path = \"/User/kbqa_portable_prj\"\n",
    "main_path = \"/temp/kbqa_portable_prj\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from functools import lru_cache, partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite_utils\n",
    "from rdflib.graph import Graph\n",
    "from rdflib_hdt import HDTStore\n",
    "from timer import timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"DP_SKIP_NLTK_DOWNLOAD\"] = \"True\"\n",
    "\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from itertools import permutations, product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deeppavlov import build_model, configs\n",
    "from deeppavlov.core.commands.infer import *\n",
    "from deeppavlov.core.commands.utils import *\n",
    "from deeppavlov.core.common.file import *\n",
    "from deeppavlov.models.kbqa.wiki_parser import *\n",
    "from rapidfuzz import fuzz\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "import inspect\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from functools import partial, reduce\n",
    "\n",
    "import editdistance\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite_utils\n",
    "import synonyms\n",
    "import torch.nn as nn\n",
    "from deeppavlov import build_model, configs\n",
    "from deeppavlov.core.commands.infer import *\n",
    "from deeppavlov.core.common.file import *\n",
    "from deeppavlov.models.kbqa.query_generator import *\n",
    "from deeppavlov.models.kbqa.query_generator_base import *\n",
    "from deeppavlov.models.kbqa.wiki_parser import *\n",
    "from pandas.io.common import _stringify_path\n",
    "from scipy.special import softmax\n",
    "from sentence_transformers import InputExample, LoggingHandler, util\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import \\\n",
    "    CECorrelationEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AdapterConfig,\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    MultiLingAdapterArguments,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import jieba\n",
    "from hashlib import sha512\n",
    "\n",
    "pd.set_option('max_colwidth', 60)\n",
    "pd.set_option(\"max_columns\", 20)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n",
    "    )\n",
    "    text_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    label_column_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "            \"efficient on GPU but very bad for TPU.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    label_all_tokens: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n",
    "            \"one (in which case the other tokens will have a padding index).\"\n",
    "        },\n",
    "    )\n",
    "    return_entity_level_metrics: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        self.task_name = self.task_name.lower()\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2732a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = os.path.join(\"/temp\", \"kbqa-explore\")\n",
    "assert os.path.exists(p2)\n",
    "sys.path.insert(0, p2)\n",
    "\n",
    "#p3 = os.path.join(main_path, \"kbqa-explore/linker_entities.pkl\")\n",
    "p3 = os.path.join(\"/temp\", \"kbqa-explore/linker_entities.pkl\")\n",
    "assert os.path.exists(p3)\n",
    "#zh_linker_entities = load_pickle(\"kbqa-explore/linker_entities.pkl\")\n",
    "zh_linker_entities = load_pickle(p3)\n",
    "zh_linker_entities.num_entities_to_return = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbed1c65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_dict_for_merge :\n",
      "{}\n",
      "srtd_with_ratios :\n",
      "[(309705, 'statement', 0, 1, 100.0, 37461), (5978728, 'Q312', 0, 1, 100.0, 8), (867427, 'Q5226961', 0, 1, 100.0, 6), (6063280, 'Q27500714', 0, 1, 100.0, 4), (611832, 'Q421253', 0, 1, 80.0, 8), (5936477, 'Q1054357', 0, 1, 80.0, 6), (4937440, 'Q1612679', 0, 1, 80.0, 5), (4535924, 'Q104819', 0, 1, 80.0, 5), (3610357, 'Q65571', 0, 1, 80.0, 4), (1864019, 'Q1144556', 0, 1, 80.0, 4), (1586669, 'Q483549', 0, 1, 80.0, 4), (769615, 'Q4781156', 0, 1, 80.0, 4), (2827822, 'Q15227818', 0, 1, 80.0, 3), (2732690, 'Q1358074', 0, 1, 80.0, 3), (1175213, 'Q305918', 0, 1, 66.66666666666667, 15), (2202251, 'Q10915001', 0, 1, 66.66666666666667, 7), (924325, 'Q504893', 0, 1, 66.66666666666667, 6), (4485018, 'Q18010946', 0, 1, 66.66666666666667, 5), (321355, 'Q213710', 0, 1, 66.66666666666667, 4), (4741273, 'Q148737', 0, 1, 66.66666666666667, 4), (1745143, 'Q270285', 0, 1, 66.66666666666667, 4), (4417578, 'Q1407528', 0, 1, 66.66666666666667, 3), (5817352, 'Q2613423', 0, 1, 66.66666666666667, 3), (1728661, 'Q3232406', 0, 1, 66.66666666666667, 3), (5476087, 'Q485192', 0, 1, 66.66666666666667, 3), (6073487, 'Q134453', 0, 1, 57.14285714285714, 4), (3070556, 'Q15235462', 0, 1, 57.14285714285714, 3), (2689804, 'Q728769', 0, 1, 57.14285714285714, 3), (5931039, 'Q18384124', 0, 1, 57.14285714285714, 3), (2583425, 'Q306345', 0, 1, 50.0, 7), (4851492, 'Q2796', 0, 1, 50.0, 4), (5327574, 'Q1228757', 0, 1, 50.0, 3), (1912918, 'Q369675', 0, 1, 50.0, 3), (4078041, 'Q24577174', 0, 1, 50.0, 3), (5509020, 'Q2984643', 0, 1, 40.0, 4), (5720416, 'Q17869020', 0, 1, 40.0, 4), (4804004, 'Q11115855', 0, 1, 40.0, 4), (232711, 'Q4437055', 0, 1, 40.0, 4), (4279211, 'Q621330', 0, 1, 33.333333333333336, 5), (343895, 'Q15903511', 0, 1, 33.333333333333336, 3), (4961293, 'Q11032526', 0, 1, 30.76923076923077, 4), (1243938, 'Q8602575', 0, 1, 28.57142857142857, 4), (166507, 'Q7850166', 0, 1, 26.66666666666667, 4), (341367, 'Q17601502', 0, 1, 25.0, 4), (3766112, 'Q7008761', 0, 1, 23.529411764705888, 4), (2644736, 'Q7145832', 0, 1, 23.529411764705888, 4), (3564575, 'Q6792905', 0, 1, 23.529411764705888, 4), (5446478, 'Q8257501', 0, 1, 19.999999999999996, 4)]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[['statement', 'Q312', 'Q5226961', 'Q27500714', 'Q421253']]],\n",
       " [[[0.01, 0.01, 0.01, 0.01, 0.01]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_linker_entities(\n",
    "    [[\"苹果\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc69caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_dict_for_merge :\n",
      "{}\n",
      "srtd_with_ratios :\n",
      "[(3045628, 'Q706815', 0, 1, 100.0, 8)]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[['Q706815']]], [[[0.01]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_linker_entities(\n",
    "    [[\"李存勖\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a23db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_dict_for_merge :\n",
      "{}\n",
      "srtd_with_ratios :\n",
      "[(3796814, 'Q485556', 0, 1, 100.0, 5)]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "output = zh_linker_entities(\n",
    "    [[\"李芳果\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02224f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q485556']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(output[0]).reshape([-1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c1074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = zh_linker_entities(\n",
    "    [[\"\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf02bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(output[0]).reshape([-1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e40ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a2db00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q216930', 'Q3244512', 'Q8337', 'Q46751', 'Q80817']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qst = \"哈利波特\"\n",
    "rep = requests.post(\n",
    "    url = \"http://localhost:8800/zh_entity_link\",\n",
    "    data = {\n",
    "        \"entity_str\":  qst\n",
    "    }\n",
    ")\n",
    "json.loads(rep.content.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca707f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbqa_env",
   "language": "python",
   "name": "kbqa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
